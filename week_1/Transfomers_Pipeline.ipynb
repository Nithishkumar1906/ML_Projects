{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4456f853-8b0b-4720-8c71-dab57b6ea4a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\hp\\anaconda3\\lib\\site-packages (4.53.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (0.33.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->transformers) (2025.4.26)\n",
      "Requirement already satisfied: torch in c:\\users\\hp\\anaconda3\\lib\\site-packages (2.7.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torch) (2025.3.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install torch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d0c8c69-03e8-47f4-8a47-dbef7fc5eb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e85e9a78-948f-4f6a-a500-5ea278d09b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "sentiment_classifier = pipeline(\"sentiment-analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8897c4a8-69fb-45d9-9af2-9d4c8da764a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998759031295776}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_classifier(\"I feel incredibly happy and grateful for all the progress I've made today.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4cfe7eb8-af0c-43dc-a14b-650a6140d1a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "NER = pipeline(\"ner\",model = \"dslim/bert-base-NER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ba80053-16c5-45aa-9d3e-6699e5ed591a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity': 'B-PER',\n",
       "  'score': np.float32(0.9817842),\n",
       "  'index': 10,\n",
       "  'word': 'A',\n",
       "  'start': 23,\n",
       "  'end': 24},\n",
       " {'entity': 'I-PER',\n",
       "  'score': np.float32(0.8869769),\n",
       "  'index': 12,\n",
       "  'word': 'P',\n",
       "  'start': 25,\n",
       "  'end': 26},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': np.float32(0.6255378),\n",
       "  'index': 13,\n",
       "  'word': '.',\n",
       "  'start': 26,\n",
       "  'end': 27},\n",
       " {'entity': 'I-PER',\n",
       "  'score': np.float32(0.70541203),\n",
       "  'index': 14,\n",
       "  'word': 'J',\n",
       "  'start': 27,\n",
       "  'end': 28},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': np.float32(0.6071143),\n",
       "  'index': 15,\n",
       "  'word': '.',\n",
       "  'start': 28,\n",
       "  'end': 29},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': np.float32(0.5517639),\n",
       "  'index': 16,\n",
       "  'word': 'Abdul',\n",
       "  'start': 30,\n",
       "  'end': 35},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': np.float32(0.7914984),\n",
       "  'index': 17,\n",
       "  'word': 'Ka',\n",
       "  'start': 36,\n",
       "  'end': 38},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': np.float32(0.94182223),\n",
       "  'index': 18,\n",
       "  'word': '##lam',\n",
       "  'start': 38,\n",
       "  'end': 41},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': np.float32(0.9221688),\n",
       "  'index': 19,\n",
       "  'word': 'Institute',\n",
       "  'start': 42,\n",
       "  'end': 51},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': np.float32(0.97511315),\n",
       "  'index': 20,\n",
       "  'word': 'of',\n",
       "  'start': 52,\n",
       "  'end': 54},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': np.float32(0.99518836),\n",
       "  'index': 21,\n",
       "  'word': 'Technology',\n",
       "  'start': 55,\n",
       "  'end': 65},\n",
       " {'entity': 'B-LOC',\n",
       "  'score': np.float32(0.9995179),\n",
       "  'index': 23,\n",
       "  'word': 'Chennai',\n",
       "  'start': 69,\n",
       "  'end': 76},\n",
       " {'entity': 'B-ORG',\n",
       "  'score': np.float32(0.9994048),\n",
       "  'index': 32,\n",
       "  'word': 'Microsoft',\n",
       "  'start': 122,\n",
       "  'end': 131},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': np.float32(0.99943537),\n",
       "  'index': 33,\n",
       "  'word': 'Corporation',\n",
       "  'start': 132,\n",
       "  'end': 143},\n",
       " {'entity': 'B-MISC',\n",
       "  'score': np.float32(0.99206513),\n",
       "  'index': 36,\n",
       "  'word': 'AI',\n",
       "  'start': 155,\n",
       "  'end': 157},\n",
       " {'entity': 'B-LOC',\n",
       "  'score': np.float32(0.9988047),\n",
       "  'index': 42,\n",
       "  'word': 'India',\n",
       "  'start': 192,\n",
       "  'end': 197},\n",
       " {'entity': 'B-LOC',\n",
       "  'score': np.float32(0.9993622),\n",
       "  'index': 50,\n",
       "  'word': 'Hyderabad',\n",
       "  'start': 239,\n",
       "  'end': 248},\n",
       " {'entity': 'B-LOC',\n",
       "  'score': np.float32(0.99938834),\n",
       "  'index': 52,\n",
       "  'word': 'Bengal',\n",
       "  'start': 250,\n",
       "  'end': 256},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': np.float32(0.9994394),\n",
       "  'index': 53,\n",
       "  'word': '##uru',\n",
       "  'start': 256,\n",
       "  'end': 259},\n",
       " {'entity': 'B-LOC',\n",
       "  'score': np.float32(0.9991837),\n",
       "  'index': 56,\n",
       "  'word': 'Pune',\n",
       "  'start': 265,\n",
       "  'end': 269}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NER(\"On July 4th, 2025, Dr. A.P.J. Abdul Kalam Institute of Technology in Chennai signed a $10 million research agreement with Microsoft Corporation to develop AI solutions for smart cities across India, with initial implementations planned in Hyderabad, Bengaluru, and Pune by March 2026.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4de2115a-e2ad-42a1-8a6c-6061b315e948",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "zeroshot_classifier = pipeline(\"zero-shot-classification\", model = \"facebook/bart-large-mnli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86401c68-e751-4758-ba0c-a2999254c593",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': 'The government has announced new environmental policies to reduce carbon emissions by 2030.',\n",
       " 'labels': ['environment', 'politics', 'technology', 'sports'],\n",
       " 'scores': [0.8612284660339355,\n",
       "  0.09613288938999176,\n",
       "  0.029138825833797455,\n",
       "  0.013499768450856209]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"The government has announced new environmental policies to reduce carbon emissions by 2030.\"\n",
    "labels = [\"politics\", \"environment\", \"sports\", \"technology\"]\n",
    "\n",
    "zeroshot_classifier(text, candidate_labels=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "adcdfdbc-6353-45c0-8f61-955d1f86d0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20d68a92-33ad-447e-ab31-6419c5615c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"bert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4f2d607-6256-45f1-a04d-94c6945c55f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8393e8d9-755e-4702-84a4-7852a81bca3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Natural Language Processing is a powerful tool used in modern AI systems.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0ed9947-6c00-4761-9a9c-0f2480ce4f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_id = tokenizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8eb4fb65-0e9f-493e-b7db-570231728e91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 3019, 2653, 6364, 2003, 1037, 3928, 6994, 2109, 1999, 2715, 9932, 3001, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b3dfa6c-cead-47ed-adfb-4af7dc7b3f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "token = tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a97763de-6a2a-441a-9852-39d2aa74f584",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'is',\n",
       " 'a',\n",
       " 'powerful',\n",
       " 'tool',\n",
       " 'used',\n",
       " 'in',\n",
       " 'modern',\n",
       " 'ai',\n",
       " 'systems',\n",
       " '.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "943e05da-e1b5-4561-92e1-7841b38c39ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_id = tokenizer.convert_tokens_to_ids(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f34e1dc6-259a-494a-85ca-6cbf49ae1581",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3019, 2653, 6364, 2003, 1037, 3928, 6994, 2109, 1999, 2715, 9932, 3001, 1012]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f24c81b2-4d29-4156-bdc6-cb53a26c14cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_id = tokenizer.decode(token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2a2a4320-93b7-47d5-9eef-1a102e44f243",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'natural language processing is a powerful tool used in modern ai systems.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c4a552-3c75-4702-93c3-66639ceb20ec",
   "metadata": {},
   "source": [
    "HUGGING FACE & PyTORCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3656525e-71f3-47dd-a00c-ea8d34163cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "508c3d97-205b-4f6e-ac07-80f68f1560f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural Language Processing is a powerful tool used in modern AI systems.\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bfa0f123-1489-4839-9088-9fea1704c108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 3019, 2653, 6364, 2003, 1037, 3928, 6994, 2109, 1999, 2715, 9932, 3001, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(input_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a83c3a98-2973-4d6f-b752-3b001065ca8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_id_PyT = tokenizer(text,return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1559def4-352a-40a7-ba82-08844a14d300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 3019, 2653, 6364, 2003, 1037, 3928, 6994, 2109, 1999, 2715, 9932,\n",
       "         3001, 1012,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_id_PyT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8b266b33-66bc-48b1-9d15-e8fc1f6ecf08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2bdb035ed194e77b3835dc4d104c9a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\hp\\.cache\\huggingface\\hub\\models--distilbert-base-uncased-finetuned-sst-2-english. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f898c9f249634b5b825c072014932fc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33eaa3d207c745cbb2275623d1265f8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1d5c41f0-bf50-4385-89dc-c6076e8fd968",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a398179d6c241059a34fb880e3b8c95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "719b528e-e8a2-4418-9fc9-5657e21593e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 3019, 2653, 6364, 2003, 1037, 3928, 6994, 2109, 1999, 2715, 9932,\n",
      "         3001, 1012,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "print(input_id_PyT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "17ca5ea5-f033-4a23-bd00-3adfa76ca246",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'POSITIVE'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if 'token_type_ids' in input_id_PyT:\n",
    "    del input_id_PyT['token_type_ids']\n",
    "with torch.no_grad():\n",
    "    logits = model(**input_id_PyT).logits\n",
    "\n",
    "predicted_class_id = logits.argmax().item()\n",
    "model.config.id2label[predicted_class_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f7a7f7-321e-46f6-bb32-19f5d449e5ce",
   "metadata": {},
   "source": [
    "SAVING THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0d0f4d5f-ec30-433e-bed5-9ff65160f5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_directory = \"My_LLM_Models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "368dd2a2-0b68-4025-a71a-6acdfb9f0549",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('My_LLM_Models\\\\tokenizer_config.json',\n",
       " 'My_LLM_Models\\\\special_tokens_map.json',\n",
       " 'My_LLM_Models\\\\vocab.txt',\n",
       " 'My_LLM_Models\\\\added_tokens.json',\n",
       " 'My_LLM_Models\\\\tokenizer.json')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(model_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4d0a8a3f-3863-489a-b7a3-02b412ff6ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(model_directory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
